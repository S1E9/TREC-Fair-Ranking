{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--groundtruth_file GROUNDTRUTH_FILE]\n",
      "                             [--query_sequence_file QUERY_SEQUENCE_FILE]\n",
      "                             [--group_annotations_file GROUP_ANNOTATIONS_FILE]\n",
      "                             [--group_definition GROUP_DEFINITION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/ericlee/Library/Jupyter/runtime/kernel-b262409e-5b7f-4ccb-bc2c-3cc16156acd1.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericlee/Library/Python/3.7/lib/python/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "    Evaluation script for the TREC Fair Ranking 2019 track.\n",
    "\"\"\"\n",
    "# --pip3 install statistics\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from statistics import mean\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "######## INPUT HANDLING ######## \n",
    "\n",
    "\n",
    "class FairRankingTask(object):\n",
    "\n",
    "    def __init__(self, sequence_file, groundtruth_file, group_annotations_file):\n",
    "\n",
    "        self.sequence = self.load_sequence_data(sequence_file)\n",
    "        self.groundtruth = self.load_groundtruth_data(groundtruth_file)\n",
    "        self.document_to_groups = self.load_document_to_groups(group_annotations_file)\n",
    "\n",
    "        self.all_groups = set([g for groups in self.document_to_groups.values() \n",
    "            for g in groups])\n",
    "\n",
    "\n",
    "    def load_sequence_data(self, input_file):\n",
    "        \"\"\"\n",
    "            Returns: a list of triples:\n",
    "                [(seq_id, q_num, q_id)]\n",
    "                1. sequence number\n",
    "                2. query number within the sequence\n",
    "                3. query id to match with the groundtruth data \n",
    "        \"\"\"\n",
    "\n",
    "        sequence_data = defaultdict(list)\n",
    "        with open(input_file, 'r') as f_in:\n",
    "            for row in csv.reader(f_in, delimiter=','):\n",
    "                seq_id, q_num = row[0].split('.')\n",
    "                # sequence_data[int(seq_id)][int(q_num)] = row[1]\n",
    "                # seq_id, q_num, q_id\n",
    "                sequence_data[int(seq_id)].append((int(q_num), int(row[1])))\n",
    "        for seq_id in sequence_data:\n",
    "            sequence_data[seq_id].sort()\n",
    "        return sequence_data\n",
    "\n",
    "\n",
    "    def load_document_to_groups(self, input_file):\n",
    "        document_to_groups = {}\n",
    "        with open(input_file, 'r') as f_in:\n",
    "            for row in csv.reader(f_in, delimiter=','):\n",
    "                document_to_groups[row[0]] = row[1:]\n",
    "\n",
    "        return document_to_groups\n",
    "\n",
    "\n",
    "    def load_groundtruth_data(self, input_file):\n",
    "        \"\"\"\n",
    "            Returns: a dictionary mapping a q_id to\n",
    "                a dictionary mapping a doc_id to the relevance value (0/1)\n",
    "                groundtruth_data[q_id][doc_id] = 0 or 1\n",
    "        \"\"\"\n",
    "\n",
    "        groundtruth_data = defaultdict(dict)\n",
    "        with open(input_file, 'r') as f_in:\n",
    "            for line in f_in:\n",
    "                query_data = json.loads(line)\n",
    "                for rel_data in query_data['documents']:\n",
    "                    groundtruth_data[query_data['qid']][rel_data['doc_id']]\\\n",
    "                        = rel_data['relevance']\n",
    "        return groundtruth_data\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def stopping_probability(cls, relevance):\n",
    "        \"\"\"\n",
    "            From participant instructions: p(s|d)=f(r_d)\n",
    "                \"this is a monotonic transform of that relevance into a probability of beingsatisfied\"\n",
    "            For this eval, we multiply relevance by a constant. \n",
    "            For future evals, we'll consider makig the stopping probabilities query dependent.\n",
    "        \"\"\"\n",
    "        return 0.7 * relevance\n",
    "\n",
    "\n",
    "    def groups_exposure(self, seq_id, submission, gamma):\n",
    "        #   The sums are aggregated differently than in the partcicipant instructions\n",
    "        #   to avoid iterating multiple times over a single ranking.\n",
    "\n",
    "        \"\"\"\n",
    "            Returns a dictionary mapping a group id to its normalized total esposure \n",
    "            in the given submission over a sequence of rankings\n",
    "        \"\"\"\n",
    "\n",
    "        g_exps = dict([(g, 0.) for g in self.all_groups])\n",
    "\n",
    "        for q_num, q_id in self.sequence[seq_id]:\n",
    "            stopped_until_now = 1.\n",
    "            for i, doc_id in enumerate(submission.rankings[seq_id][q_num]):\n",
    "                # for author in self.document_to_authors[doc_id]:\n",
    "                if doc_id not in self.document_to_groups:\n",
    "                    # logging.warning(\"Doc %s not not mapped to a group\" % doc_id)\n",
    "                    continue\n",
    "                for g in self.document_to_groups[doc_id]:\n",
    "                    g_exps[g] += (gamma ** i) * stopped_until_now * \\\n",
    "                        FairRankingTask.stopping_probability(self.groundtruth[q_id][doc_id])\n",
    "                stopped_until_now *= (1 - \n",
    "                    FairRankingTask.stopping_probability(self.groundtruth[q_id][doc_id]))\n",
    "\n",
    "        total = sum(g_exps.values())\n",
    "        return dict([(g, exp / total) for g, exp in g_exps.items()])\n",
    "\n",
    "\n",
    "    def groups_relevance(self, seq_id, submission):\n",
    "        #   The sums are aggregated differently than in the partcicipant instructions\n",
    "        #   to avoid iterating multiple times over a single ranking.\n",
    "\n",
    "        \"\"\"\n",
    "            Returns a dictionary mapping a group id to its normalized total relevance \n",
    "            in the given submission over a sequence of rankings\n",
    "        \"\"\"\n",
    "\n",
    "        g_rels = dict([(g, 0.) for g in self.all_groups])\n",
    "\n",
    "        # Important to iterate over our original sequence\n",
    "        # to avoid evaluating over manipulated sequences\n",
    "        for q_num, q_id in self.sequence[seq_id]:\n",
    "            for doc_id in submission.rankings[seq_id][q_num]:\n",
    "                if doc_id not in self.document_to_groups:\n",
    "                    # logging.warning(\"Doc %s not not mapped to a group\" % doc_id)\n",
    "                    continue\n",
    "                for g in self.document_to_groups[doc_id]:\n",
    "                    g_rels[g] += FairRankingTask.stopping_probability(self.groundtruth[q_id][doc_id])\n",
    "\n",
    "        total = sum(g_rels.values())\n",
    "        return dict([(g, rel / total) for g, rel in g_rels.items()])\n",
    "\n",
    "\n",
    "\n",
    "class FairRankingSubmission(object):\n",
    "\n",
    "    def __init__(self, run_file):\n",
    "        self.rankings = self.load_submission(run_file)\n",
    "\n",
    "\n",
    "    def load_submission(self, input_file):\n",
    "        \"\"\"\n",
    "            Returns: a dictionary mapping a q_id to\n",
    "                a dictionary mapping a doc_id to the relevance value (0/1)\n",
    "                groundtruth_data[q_id][doc_id] = 0 or 1\n",
    "        \"\"\"\n",
    "\n",
    "        submission_data = defaultdict(dict)\n",
    "        with open(input_file, 'r') as f_in:\n",
    "            for line in f_in:\n",
    "                ranking_data = json.loads(line)\n",
    "                #note: this strips the original query ID, as it will be read from the groundtruth data\n",
    "                # to avoid manipulations\n",
    "                seq_id, q_num = ranking_data['q_num'].split('.')\n",
    "                submission_data[int(seq_id)][int(q_num)] = ranking_data['ranking']\n",
    "        return submission_data\n",
    "\n",
    "\n",
    "\n",
    "######## UTILITY METRICS ######## \n",
    "\n",
    "\n",
    "def expected_utility(ranking, groundtruth, gamma):\n",
    "\n",
    "    \"\"\"\n",
    "        Assumes ranking is a participant-sorted list of documents\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    u = 0.\n",
    "    stopped_until_now = 1.\n",
    "    for i, doc_id in enumerate(ranking):\n",
    "        stop_p = FairRankingTask.stopping_probability(groundtruth[doc_id])\n",
    "        u += (gamma ** i) * stopped_until_now * stop_p\n",
    "        stopped_until_now *= (1 - stop_p)\n",
    "    return u\n",
    "\n",
    "\n",
    "def avg_expected_utility(seq_id, task, submission, gamma):\n",
    "\n",
    "    \"\"\"\n",
    "        Assumes sequence is sorted by sequence number, \n",
    "        then by the query number within the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    return sum([expected_utility(\n",
    "        submission.rankings[seq_id][q_num], task.groundtruth[q_id], gamma) \n",
    "        for q_num, q_id in task.sequence[seq_id]]) / len(task.sequence[seq_id])\n",
    "\n",
    "\n",
    "######## FAIRNESS METRICS ######## \n",
    "\n",
    "def l2_loss(seq_id, task, submission, gamma):\n",
    "    groups_exposure = task.groups_exposure(seq_id, submission, gamma)\n",
    "    groups_relevance = task.groups_relevance(seq_id, submission)\n",
    "    return math.sqrt(sum([(groups_exposure[g] - groups_relevance[g])**2 \n",
    "        for g in task.all_groups]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \"\"\"\n",
    "         python3 trec-fair-ranking-evaluator.py  \\\n",
    "            --groundtruth_file TREC-Competition-eval-sample-with-rel.json  \\\n",
    "            --query_sequence_file TREC-Competition-eval-seq-5-25000.csv \\\n",
    "            --group_annotations_file group_annotations/article-IMFLevel.csv \\\n",
    "            --group_definition IMFLevel\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Evaluate a TREC Fair Ranking submission.')\n",
    "    parser.add_argument('--groundtruth_file', help='fair ranking ground truth file')\n",
    "    parser.add_argument('--query_sequence_file', help='fair ranking query sequences file')\n",
    "    parser.add_argument('--group_annotations_file', help='document group annotations file')\n",
    "    parser.add_argument('--group_definition', help='keyword defininf group definitions')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    task = FairRankingTask(args.query_sequence_file, args.groundtruth_file, args.group_annotations_file)\n",
    "\n",
    "\n",
    "    run_files_prefix = 'fairRuns/'\n",
    "    run_files = [\n",
    "        # 'example_run_name_1', \n",
    "        # 'example_run_name_2', \n",
    "        \"dummy_run_1_1.json\"\n",
    "    ]\n",
    "\n",
    "    performance_all_utility = defaultdict(list)\n",
    "    performance_team_utility = defaultdict(dict)\n",
    "    performance_all_fairness = defaultdict(list)\n",
    "    performance_team_fairness = defaultdict(dict)\n",
    "\n",
    "    for run in run_files:\n",
    "        submission = FairRankingSubmission(run_files_prefix + run)\n",
    "\n",
    "        for seq_id in task.sequence:\n",
    "            fairness = l2_loss(seq_id, task, submission, gamma=0.5)\n",
    "            utility = avg_expected_utility(seq_id, task, submission, gamma=0.5)\n",
    "\n",
    "            performance_team_fairness[run][seq_id] = fairness\n",
    "            performance_team_utility[run][seq_id] = utility\n",
    "            performance_all_fairness[seq_id].append(fairness)\n",
    "            performance_all_utility[seq_id].append(utility)\n",
    "\n",
    "    for run in run_files:\n",
    "        with open('eval_results/%s/%s' % (args.group_definition, run), 'w') as f_out:\n",
    "            f_out.write('seq_id\\tutil-min\\tutil-max\\tutil-mean\\tutil-run\\tunfairness-min\\tunfairness-max\\tunfairness-mean\\tunfairness-run\\n')\n",
    "            for seq_id in sorted(task.sequence):\n",
    "                f_out.write('%d\\t%f\\t%f\\t%f\\t%f\\t%f\\t%f\\t%f\\t%f\\n' % (\n",
    "                    seq_id,\n",
    "                    min(performance_all_utility[seq_id]),\n",
    "                    max(performance_all_utility[seq_id]),\n",
    "                    mean(performance_all_utility[seq_id]),\n",
    "                    performance_team_utility[run][seq_id], # run's utility\n",
    "                    min(performance_all_fairness[seq_id]),\n",
    "                    max(performance_all_fairness[seq_id]),\n",
    "                    mean(performance_all_fairness[seq_id]),\n",
    "                    performance_team_fairness[run][seq_id], # run's unfairness\n",
    "                    ))\n",
    "\n",
    "\n",
    "    colormap = plt.cm.gist_ncar \n",
    "    colors = [colormap(i) for i in np.linspace(0, 0.9, len(run_files))]\n",
    "    for i, run in enumerate(run_files):\n",
    "        plt.scatter(x=[mean(performance_team_fairness[run].values())], \n",
    "            y=[mean(performance_team_utility[run].values())], c=[colors[i]],\n",
    "            label=run, s=50)\n",
    "\n",
    "    plt.legend(loc='lower right', fontsize='xx-small')\n",
    "    plt.xlabel(\"Unfairness: L2\")\n",
    "    plt.ylabel(\"Utility: expected utility\")\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    plt.savefig('plots/%s/performance-all.pdf' % args.group_definition)\n",
    "    # plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
