{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(topics_file):\n",
    "    queries = {}\n",
    "    with open(topics_file, 'rt') as f:\n",
    "        for line in f:\n",
    "            q = json.loads(line)\n",
    "            qid = q[\"qid\"]\n",
    "            queries[qid] = q\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_sets(topics_file):\n",
    "    topic_doc_sets = {}\n",
    "    queries = get_topics(topics_file)\n",
    "    for id, query in queries.items():\n",
    "        doc_set = set([])\n",
    "        docs = query['documents']\n",
    "        for d in docs:\n",
    "            doc_set.add(d['doc_id'])\n",
    "        topic_doc_sets[id] = doc_set\n",
    "    return topic_doc_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' refine PyTerrier rankings to: include missing docs, exclude docs not in candidate set, query scores sum to 1 \n",
    "\n",
    "    you will also have to check that all the topics (qids) have documents in the ranking. \n",
    "    There was at least 1 topic that Terrier returned no documents for. You can just assign random relevance \n",
    "    scores for documents for queries that have no documents returned.     \n",
    "'''\n",
    "def get_relevance(res_location, topics_file):\n",
    "    final_ranking = []\n",
    "    candidates = candidate_sets(topics_file)\n",
    "    current_topic = '0'\n",
    "    with open(res_location) as f:\n",
    "        topic_rank = []\n",
    "        topic_scores = []\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            topic = int(parts[0])\n",
    "            docno = parts[2]\n",
    "            score = float(parts[4])\n",
    "            \n",
    "            if topic != current_topic:\n",
    "                '''first document of topic found'''\n",
    "                if current_topic != '0':\n",
    "                    '''check remaining docs at end of a topic'''\n",
    "                    for doc in topic_candidates:\n",
    "                        topic_rank.append(doc)\n",
    "                        topic_scores.append(topic_scores[-1]*0.9)\n",
    "                    tscore = sum(topic_scores)\n",
    "                    topic_scores = [w/tscore for w in topic_scores]\n",
    "                    for i, d in enumerate(topic_rank):\n",
    "                        final_ranking.append(str(current_topic) + \" Q0 \" + d + \" \" + str(i) + \" \" + str(topic_scores[i]) + \" \" + parts[5])\n",
    "                    \n",
    "                current_topic = topic\n",
    "                topic_candidates = candidates[current_topic]\n",
    "                topic_rank = []\n",
    "                topic_scores = []\n",
    "                \n",
    "                inset = docno in topic_candidates\n",
    "                if inset:\n",
    "                    topic_rank.append(docno)\n",
    "                    topic_scores.append(score)\n",
    "                    topic_candidates.remove(docno)\n",
    "            \n",
    "            else:\n",
    "                inset = docno in topic_candidates\n",
    "                if inset:\n",
    "                    topic_rank.append(docno)\n",
    "                    topic_scores.append(score)\n",
    "                    topic_candidates.remove(docno)\n",
    "\n",
    "    for doc in topic_candidates:\n",
    "        topic_rank.append(doc)\n",
    "        topic_scores.append(topic_scores[-1]*0.9)\n",
    "    tscore = sum(topic_scores)\n",
    "    topic_scores = [w/tscore for w in topic_scores]\n",
    "    for i, d in enumerate(topic_rank):\n",
    "        final_ranking.append(str(current_topic) + \" Q0 \" + d + \" \" + str(i) + \" \" + str(topic_scores[i]) + \" \" + parts[5])\n",
    "            \n",
    "    return final_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_res(res, res_path):\n",
    "    with open(res_path, 'wt') as f:\n",
    "        for inst in res:\n",
    "            f.write(inst + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = get_relevance(\"path/to/your/PyTerrier.res\", \"path/to/your/TREC-Fair-Ranking-training-sample.json\")\n",
    "write_res(ranking, \"path/to/save/new.res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
